\section{Conclusion}
The work described in this paper describes the methodology in applying deep learning to the task of determining the state of a smartphone on the user's person. 
To do this, we do not require the user to be performing any specific action.
Instead, we utilize data from the accelerometer and screen, which are both lightweight and readily available on Android phone models, to identify four common phone positions that span most of a user's phone behavior
and appear distinguishable from the sensor data. 
We show that great accuracy can be achieved when evaluating on a single phone model.
Furthermore, we propose an accelerometer calibration strategy for standardizing phone accelerometer
data across phone models, and show the potential generalization of a network trained on data from a single phone model to other phone models.
 However, our cross model results are inconclusive. 
 For future work, we plan to strengthen these findings with the collection of more data, specifically the hand and pocket cases as well as investigate the classifier across phone models.
We would also like to experiment with the `smoothing' of outputs and determine if it can enhance accuracy.