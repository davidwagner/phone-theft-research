\section{Conclusion}
The work described in this paper describes the methodology in applying deep learning to the task of determining the state of a user's smartphone on the user's person. 
To do this, we do not require the user to be performing any specific action.
Instead, we utilize data from the accelerometer and screen, which are both lightweight and readily available on Android phone models, to identify four common phone positions that span most of a user's phone behavior
and appear distinguishable from the sensor data. 
We show that great accuracy can be achieved when evaluating on a single phone model.
Furthermore, we propose an accelerometer calibration strategy for standardizing phone accelerometer
data across phone models, and show the potential generalization of a network trained on data from a single phone model to other phone models. But our cross model results are inconclusive.