\section{Related work}
There has been much previous work involving smartphone sensors.
Note that this is a different set of work than the previous work that exists for wearable sensors, \cite{Kunze2005,Atallah}.
We focus on smartphone sensors since we believe they are more readily accessible, prevalent, and versatile than wearable sensors.

Work by Khan \et, utilized accelerometer values to classify phone state 
but was limited to distinguishing between whether the phone was in the upper or lower half of the body \cite{Khan2010}.
Similarly, Miluzzo \et proposed the use of various sensors to classify different phone locations, 
but only recognized two states: inside and outside of a pocket \cite{Miluzzo2010}.
In contrast, our work distinguishes between four states: hand, pocket, backpack, and table. 

Several other works tried to classify and distinguish between more states (e.g. hand, bag, pocket, etc.), but only used data from the accelerometer resulting in accuracies between 74.6\% to 84\% \cite{Fujinami2013,Coksun15}. 
Our work improves on this accuracy by utilizing additional data from the phone screen.

Park \et achieved an accuracy of 94\% in distinguishing between hand, ear, pocket, and backpack, but limited the data to record instances where the user was walking \cite{Park2012}. 
In contrast, our work does not require that the phone user perform a specific task in order to distinguish between states.
Rather, we train and validate our model on phone data collected throughout the entire day, 
 including times when the phone may be still and not actually on the user. 

Other works improved accuracy by using several additional sensors \cite{Yang13}.
For example, Wiese \et explored the idea of adding sensors that utilize capacitive sensing,  multi-spectral properties, as well as light and proximity sensing \cite{Wiese2013}.
They were able to achieve accuracies of 85\% to 100\%.
Similarly, Mart\'{\i}n \et used sensors like light, proximity, and acceleration sensors
to obtain phone state accuracy of 92.94\% \cite{Martin2013}, but at a cost in file size.
Our work achieves similar accuracy rates but with data from fewer sensors.  

In a somewhat different approach, work by Wijerathne \et \cite{Wijerathne} took advantage of smartphone accelerometers to help monitor road conditions. Similar to our work, the paper attempted to generalize between all positions the phone could be in, but instead of detecting the exact position of the user's phone, the authors attempted to detect rough roads and bumps while cycling. 

In this paper, we take this idea further and utilize data from only two sources: the accelerometer and the phone screen. 
Data from both sources is readily available, does not require specific user permissions, and is less energy draining than previously studied sensors.
Furthermore, unlike previous work, we also train and validate our model on phone data collected throughout the entire day, and not just during specific tasks (e.g. walking).
This includes times when the phone may be still and not actually directly on the user.
An example of this may be if the phone is left on a table or in a backpack. 







